#!/usr/bin/env python3
"""
Digest Auto CLI
===============

å¥å…¨æ€§è¨ºæ–­CLIã€‚Claudeã‹ã‚‰å‘¼ã³å‡ºã•ã‚Œã€ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹ã‚’åˆ†æã—ã€
ã¾ã ã‚‰ãƒœã‚±ã‚’æ¤œå‡ºã€ç”Ÿæˆå¯èƒ½ãªãƒ€ã‚¤ã‚¸ã‚§ã‚¹ãƒˆéšå±¤ã‚’æ¨å¥¨ã™ã‚‹ã€‚

Usage:
    python -m interfaces.digest_auto --output json
    python -m interfaces.digest_auto --output text
"""

import argparse
import json
import sys
from dataclasses import asdict, dataclass, field
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

from infrastructure.json_repository import load_json, try_load_json

from interfaces.cli_helpers import output_error, output_json

from domain.exceptions import FileIOError
from domain.file_constants import (
    CONFIG_FILENAME,
    DIGEST_TIMES_FILENAME,
    GRAND_DIGEST_FILENAME,
    PLUGIN_CONFIG_DIR,
    SHADOW_GRAND_DIGEST_FILENAME,
)


@dataclass
class Issue:
    """æ¤œå‡ºã•ã‚ŒãŸå•é¡Œ"""

    type: str  # "unprocessed_loops" | "placeholders" | "gaps"
    level: Optional[str] = None
    count: int = 0
    files: List[str] = field(default_factory=list)
    details: Optional[Dict[str, Any]] = None


@dataclass
class LevelStatus:
    """éšå±¤ã®çŠ¶æ…‹"""

    level: str
    current: int
    threshold: int
    ready: bool
    source_type: str  # "loops" | level name


@dataclass
class AnalysisResult:
    """åˆ†æçµæœ"""

    status: str  # "ok" | "warning" | "error"
    issues: List[Issue] = field(default_factory=list)
    generatable_levels: List[LevelStatus] = field(default_factory=list)
    insufficient_levels: List[LevelStatus] = field(default_factory=list)
    recommendations: List[str] = field(default_factory=list)
    error: Optional[str] = None


class DigestAutoAnalyzer:
    """å¥å…¨æ€§è¨ºæ–­ã‚¯ãƒ©ã‚¹"""

    # éšå±¤ã®è¦ªå­é–¢ä¿‚
    LEVEL_HIERARCHY = {
        "weekly": {"source": "loops", "threshold_key": "weekly_threshold"},
        "monthly": {"source": "weekly", "threshold_key": "monthly_threshold"},
        "quarterly": {"source": "monthly", "threshold_key": "quarterly_threshold"},
        "annual": {"source": "quarterly", "threshold_key": "annual_threshold"},
        "triennial": {"source": "annual", "threshold_key": "triennial_threshold"},
        "decadal": {"source": "triennial", "threshold_key": "decadal_threshold"},
        "multi_decadal": {"source": "decadal", "threshold_key": "multi_decadal_threshold"},
        "centurial": {"source": "multi_decadal", "threshold_key": "centurial_threshold"},
    }

    LEVEL_ORDER = [
        "weekly",
        "monthly",
        "quarterly",
        "annual",
        "triennial",
        "decadal",
        "multi_decadal",
        "centurial",
    ]

    def __init__(self, plugin_root: Optional[Path] = None):
        """
        Args:
            plugin_root: Pluginãƒ«ãƒ¼ãƒˆï¼ˆæŒ‡å®šã—ãªã„å ´åˆã¯è‡ªå‹•æ¤œå‡ºã‚’è©¦ã¿ã‚‹ï¼‰
        """
        if plugin_root:
            self.plugin_root = Path(plugin_root).resolve()
        else:
            self.plugin_root = Path(__file__).resolve().parent.parent.parent

        self.config_file = self.plugin_root / PLUGIN_CONFIG_DIR / CONFIG_FILENAME
        self.last_digest_file = self.plugin_root / PLUGIN_CONFIG_DIR / DIGEST_TIMES_FILENAME

    def _load_config(self) -> Dict[str, Any]:
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
        return load_json(self.config_file)

    def _resolve_base_dir(self, config: Dict[str, Any]) -> Path:
        """base_dirã‚’è§£æ±º"""
        base_dir_str = config.get("base_dir", ".")
        base_path = Path(base_dir_str).expanduser()
        if not base_path.is_absolute():
            base_path = self.plugin_root / base_path
        return base_path.resolve()

    def _load_json_file(self, file_path: Path) -> Optional[Dict[str, Any]]:
        """JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€ï¼ˆå­˜åœ¨ã—ãªã„å ´åˆã¯Noneï¼‰"""
        return try_load_json(file_path, log_on_error=False)

    def _extract_file_number(self, filename: str) -> Optional[int]:
        """ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ç•ªå·ã‚’æŠ½å‡º"""
        import re

        # L00001, W0001, M001 ãªã©ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ãƒãƒƒãƒ
        match = re.search(r"[A-Z]+(\d+)", filename)
        if match:
            return int(match.group(1))
        return None

    def _find_gaps(self, numbers: List[int]) -> List[int]:
        """é€£ç•ªã®ã‚®ãƒ£ãƒƒãƒ—ã‚’æ¤œå‡º"""
        if len(numbers) < 2:
            return []

        sorted_nums = sorted(numbers)
        gaps = []
        for i in range(len(sorted_nums) - 1):
            for n in range(sorted_nums[i] + 1, sorted_nums[i + 1]):
                gaps.append(n)
        return gaps

    def analyze(self) -> AnalysisResult:
        """åˆ†æå®Ÿè¡Œ"""
        issues: List[Issue] = []
        recommendations: List[str] = []

        try:
            # 1. è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
            config = self._load_config()
            base_dir = self._resolve_base_dir(config)
            paths = config.get("paths", {})
            levels_config = config.get("levels", {})

            loops_path = base_dir / paths.get("loops_dir", "data/Loops")
            essences_path = base_dir / paths.get("essences_dir", "data/Essences")
            digests_path = base_dir / paths.get("digests_dir", "data/Digests")

            # 2. æœªå‡¦ç†Loopæ¤œå‡º
            unprocessed_loops = self._check_unprocessed_loops(loops_path)
            if unprocessed_loops:
                issues.append(
                    Issue(
                        type="unprocessed_loops",
                        count=len(unprocessed_loops),
                        files=unprocessed_loops,
                    )
                )
                recommendations.append("Run /digest to process unprocessed loops first")

            # 3. ShadowGrandDigestç¢ºèª
            shadow_file = essences_path / SHADOW_GRAND_DIGEST_FILENAME
            shadow_data = self._load_json_file(shadow_file)
            if shadow_data is None:
                return AnalysisResult(
                    status="error",
                    error="ShadowGrandDigest.txt not found or corrupted",
                    recommendations=["Run @digest-setup to initialize"],
                )

            # 4. ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼æ¤œå‡º
            placeholders = self._check_placeholders(shadow_data)
            if placeholders:
                for level, files in placeholders:
                    issues.append(
                        Issue(
                            type="placeholders",
                            level=level,
                            count=len(files),
                            files=files,
                        )
                    )
                recommendations.append("Run /digest to complete pending analysis")

            # 5. ä¸­é–“ãƒ•ã‚¡ã‚¤ãƒ«ã‚¹ã‚­ãƒƒãƒ—æ¤œå‡ºï¼ˆè­¦å‘Šã®ã¿ï¼‰
            gaps = self._check_gaps(shadow_data)
            if gaps:
                for level, gap_info in gaps.items():
                    issues.append(
                        Issue(
                            type="gaps",
                            level=level,
                            count=len(gap_info["missing"]),
                            details=gap_info,
                        )
                    )
                recommendations.append("Consider adding missing files to prevent memory gaps")

            # 6. GrandDigestç¢ºèªã¨ç”Ÿæˆå¯èƒ½ãªéšå±¤åˆ¤å®š
            grand_file = essences_path / GRAND_DIGEST_FILENAME
            grand_data = self._load_json_file(grand_file) or {}

            generatable, insufficient = self._determine_generatable_levels(
                config=config,
                loops_path=loops_path,
                digests_path=digests_path,
                grand_data=grand_data,
                unprocessed_count=len(unprocessed_loops),
            )

            # æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®è¿½åŠ 
            if generatable:
                # ä¸‹ä½éšå±¤ã‹ã‚‰é †ã«æ¨å¥¨
                for level_status in generatable:
                    recommendations.append(f"Run /digest {level_status.level} to generate digest")

            # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åˆ¤å®š
            if unprocessed_loops or placeholders:
                status = "warning"
            elif generatable:
                status = "ok"
            else:
                status = "ok"

            return AnalysisResult(
                status=status,
                issues=issues,
                generatable_levels=generatable,
                insufficient_levels=insufficient,
                recommendations=recommendations,
            )

        except FileIOError as e:
            return AnalysisResult(
                status="error",
                error=str(e),
                recommendations=["Run @digest-setup first"],
            )
        except Exception as e:
            return AnalysisResult(
                status="error",
                error=str(e),
            )

    def _check_unprocessed_loops(self, loops_path: Path) -> List[str]:
        """æœªå‡¦ç†Loopæ¤œå‡º"""
        if not loops_path.exists():
            return []

        # Loopãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
        loop_files = list(loops_path.glob("L*.txt"))
        if not loop_files:
            return []

        # last_processed ã‚’å–å¾—
        last_digest_data = self._load_json_file(self.last_digest_file)
        last_processed = None
        if last_digest_data:
            weekly_data = last_digest_data.get("weekly", {})
            last_processed = weekly_data.get("last_processed")

        # last_processedã‚ˆã‚Šå¾Œã®Loopã‚’æ¤œå‡º
        unprocessed = []
        for f in loop_files:
            file_num = self._extract_file_number(f.stem)
            if file_num is not None:
                if last_processed is None or file_num > last_processed:
                    unprocessed.append(f.stem)

        return sorted(unprocessed)

    def _check_placeholders(self, shadow_data: Dict[str, Any]) -> List[Tuple[str, List[str]]]:
        """ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼æ¤œå‡º"""
        placeholders = []
        latest_digests = shadow_data.get("latest_digests", {})

        for level in self.LEVEL_ORDER:
            level_data = latest_digests.get(level, {})
            overall_digest = level_data.get("overall_digest")

            if overall_digest is not None:
                source_files = overall_digest.get("source_files", [])
                # source_filesãŒã‚ã‚‹ã®ã«abstractãŒãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã®å ´åˆ
                abstract = overall_digest.get("abstract", "")
                if source_files and isinstance(abstract, str) and "<!-- PLACEHOLDER" in abstract:
                    placeholders.append((level, source_files))

        return placeholders

    def _check_gaps(self, shadow_data: Dict[str, Any]) -> Dict[str, Dict[str, Any]]:
        """ä¸­é–“ãƒ•ã‚¡ã‚¤ãƒ«ã‚¹ã‚­ãƒƒãƒ—æ¤œå‡º"""
        gaps = {}
        latest_digests = shadow_data.get("latest_digests", {})

        for level in self.LEVEL_ORDER:
            level_data = latest_digests.get(level, {})
            overall_digest = level_data.get("overall_digest")

            if overall_digest is not None:
                source_files = overall_digest.get("source_files", [])
                if len(source_files) > 1:
                    numbers = []
                    for f in source_files:
                        num = self._extract_file_number(f)
                        if num is not None:
                            numbers.append(num)

                    if numbers:
                        missing = self._find_gaps(numbers)
                        if missing:
                            gaps[level] = {
                                "range": f"{source_files[0]}ï½{source_files[-1]}",
                                "missing": missing,
                            }

        return gaps

    def _determine_generatable_levels(
        self,
        config: Dict[str, Any],
        loops_path: Path,
        digests_path: Path,
        grand_data: Dict[str, Any],
        unprocessed_count: int,
    ) -> Tuple[List[LevelStatus], List[LevelStatus]]:
        """ç”Ÿæˆå¯èƒ½ãªéšå±¤åˆ¤å®š"""
        levels_config = config.get("levels", {})
        major_digests = grand_data.get("major_digests", {})

        generatable = []
        insufficient = []

        # å„éšå±¤ã®ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        level_counts: Dict[str, int] = {}

        for level in self.LEVEL_ORDER:
            hierarchy = self.LEVEL_HIERARCHY[level]
            source = hierarchy["source"]
            threshold_key = hierarchy["threshold_key"]
            threshold = levels_config.get(threshold_key, 5)

            if source == "loops":
                # Loopãƒ•ã‚¡ã‚¤ãƒ«æ•°ï¼ˆæœªå‡¦ç†å«ã‚€ï¼‰
                if loops_path.exists():
                    current = len(list(loops_path.glob("L*.txt")))
                else:
                    current = 0
            else:
                # ä¸‹ä½éšå±¤ã®Regular Digestæ•°
                source_level_data = major_digests.get(source, {})
                overall = source_level_data.get("overall_digest")
                if overall:
                    # GrandDigestã«ã‚ã‚‹ = ç¢ºå®šæ¸ˆã¿
                    # å®Ÿéš›ã®ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
                    source_dir = self._get_level_dir(digests_path, source)
                    if source_dir.exists():
                        # Provisionalä»¥å¤–ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
                        current = len(
                            [f for f in source_dir.glob("*.txt") if "Provisional" not in str(f.parent)]
                        )
                    else:
                        current = 0
                else:
                    current = 0

            level_counts[level] = current

            status = LevelStatus(
                level=level,
                current=current,
                threshold=threshold,
                ready=current >= threshold,
                source_type=source,
            )

            if current >= threshold:
                generatable.append(status)
            else:
                insufficient.append(status)

        return generatable, insufficient

    def _get_level_dir(self, digests_path: Path, level: str) -> Path:
        """éšå±¤ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹ã‚’å–å¾—"""
        level_dirs = {
            "weekly": "1_Weekly",
            "monthly": "2_Monthly",
            "quarterly": "3_Quarterly",
            "annual": "4_Annual",
            "triennial": "5_Triennial",
            "decadal": "6_Decadal",
            "multi_decadal": "7_Multi-decadal",
            "centurial": "8_Centurial",
        }
        return digests_path / level_dirs.get(level, level)


def format_text_report(result: AnalysisResult) -> str:
    """ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ã§ãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆãƒ†ã‚¹ãƒˆå¯èƒ½ï¼‰

    Args:
        result: åˆ†æçµæœ

    Returns:
        ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆ
    """
    output = []
    output.append("```text")
    output.append("â”" * 40)
    output.append("ğŸ“Š EpisodicRAG ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹")
    output.append("â”" * 40)
    output.append("")

    # ã‚¨ãƒ©ãƒ¼ã®å ´åˆ
    if result.status == "error":
        output.append(f"âŒ ã‚¨ãƒ©ãƒ¼: {result.error}")
        if result.recommendations:
            output.append("")
            for rec in result.recommendations:
                output.append(f"  â†’ {rec}")
        output.append("")
        output.append("â”" * 40)
        output.append("```")
        return "\n".join(output)

    # å•é¡Œã®è¡¨ç¤º
    if result.issues:
        for issue in result.issues:
            if issue.type == "unprocessed_loops":
                output.append(f"âš ï¸ æœªå‡¦ç†Loopæ¤œå‡º: {issue.count}å€‹")
                for f in issue.files[:5]:  # æœ€å¤§5ä»¶è¡¨ç¤º
                    output.append(f"  - {f}")
                if len(issue.files) > 5:
                    output.append(f"  ... ä»–{len(issue.files) - 5}å€‹")
                output.append("")

            elif issue.type == "placeholders":
                output.append(f"âš ï¸ ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼æ¤œå‡º ({issue.level}): {issue.count}å€‹")
                output.append("")

            elif issue.type == "gaps":
                output.append(f"âš ï¸ ä¸­é–“ãƒ•ã‚¡ã‚¤ãƒ«ã‚¹ã‚­ãƒƒãƒ— ({issue.level})")
                if issue.details:
                    output.append(f"  ç¯„å›²: {issue.details.get('range', '')}")
                    missing = issue.details.get("missing", [])
                    output.append(f"  æ¬ ç•ª: {len(missing)}å€‹")
                output.append("")

    # ç”Ÿæˆå¯èƒ½ãªéšå±¤
    if result.generatable_levels:
        output.append("âœ… ç”Ÿæˆå¯èƒ½ãªãƒ€ã‚¤ã‚¸ã‚§ã‚¹ãƒˆ")
        for level in result.generatable_levels:
            output.append(f"  âœ… {level.level} ({level.current}/{level.threshold})")
        output.append("")

    # ä¸è¶³ã—ã¦ã„ã‚‹éšå±¤
    if result.insufficient_levels:
        output.append("â³ ç”Ÿæˆã«å¿…è¦ãªãƒ•ã‚¡ã‚¤ãƒ«æ•°")
        for level in result.insufficient_levels:
            need = level.threshold - level.current
            output.append(f"  âŒ {level.level} ({level.current}/{level.threshold}) - ã‚ã¨{need}å€‹å¿…è¦")
        output.append("")

    # æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
    if result.recommendations:
        output.append("â”" * 40)
        output.append("ğŸ“ˆ æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³")
        output.append("â”" * 40)
        for i, rec in enumerate(result.recommendations, 1):
            output.append(f"  {i}. {rec}")
        output.append("")

    output.append("â”" * 40)
    output.append("```")
    return "\n".join(output)


def print_text_report(result: AnalysisResult) -> None:
    """ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ã§ãƒ¬ãƒãƒ¼ãƒˆã‚’å‡ºåŠ›ï¼ˆVSCodeå¯¾å¿œï¼‰"""
    print(format_text_report(result))


def main(plugin_root: Optional[Path] = None) -> None:
    """CLIã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ"""
    parser = argparse.ArgumentParser(
        description="EpisodicRAG Health Diagnostic",
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )

    parser.add_argument(
        "--output",
        choices=["json", "text"],
        default="json",
        help="Output format (default: json)",
    )

    parser.add_argument(
        "--plugin-root",
        type=Path,
        help="Override plugin root (for testing)",
    )

    args = parser.parse_args()

    # plugin_rootã®æ±ºå®š
    effective_root = args.plugin_root if args.plugin_root else plugin_root

    try:
        analyzer = DigestAutoAnalyzer(plugin_root=effective_root)
        result = analyzer.analyze()

        if args.output == "json":
            output_json(asdict(result))
        else:
            print_text_report(result)

    except Exception as e:
        output_error(str(e))


if __name__ == "__main__":
    import io

    # Windows UTF-8å¯¾å¿œ
    if sys.platform == "win32":
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding="utf-8")
        sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding="utf-8")

    main()
